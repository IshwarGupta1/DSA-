Distributed Job Scheduler---


Client ---> Load Balancer --> Submission Service ---> DataStore --> Scheduler Service --> RabbitMQ/Kafka --> Execution Service

Submission Service deals with start job cancel job or update job
DataStore contains different DB -
Job Table = job metadata
Job Execution Table = Job status
Job Schedules Table = Scheduled runtime and next runtime
Workers Table = Workers details with job

Scheduler Service - jobs selected and executed based on runtime given


System API Design
Here are some of the important API’s we can have in our system.

1. Submit Job (POST /jobs)

2. Get Job Status (GET /jobs/{job_id})

3. Cancel Job (DELETE /jobs/{job_id})

4. List Pending Jobs (GET /jobs?status=pending&user_id=u003)

5. Get Jobs Running on a Worker (GET /job/executions?worker_id=w001)


Deep Dive into Key Components
4.1 SQL vs NoSQL
To choose the right database for our needs, let's consider some factors that can affect our choice:

We need to store millions of jobs every day.

Read and Write queries are around the same.

Data is structured with fixed schema.

We don’t require ACID transactions or complex joins.

Both SQL and NoSQL databases could meet these needs, but given the scale and nature of the workload, a NoSQL database like DynamoDB or Cassandra could be a better fit, especially when handling millions of jobs per day and supporting high-throughput writes and reads.
